---
title: "Homework #6: Clustering" 
author: "**Siyu Jian / sj9va**"
date: "Due: Mon Apr 5 | 10:55am"
output: R6018::homework
---

**SYS 4582/6018 | Spring 2021 | University of Virginia **

*******************************************
```{r config, echo=FALSE}
source(system.file("config/hw_config.R", package="R6018")) # knitr settings
# options(dplyr.summarise.inform = FALSE)  # ignore dplyr message about grouping
```


# Required R packages and Directories

### {.solution}
```{r packages, message=FALSE, warning=FALSE}
data.dir = 'https://mdporter.github.io/SYS6018/data/' # data directory
library(R6018)     # functions for SYS-6018
library(tidyverse) # functions for data manipulation   
library(mclust)    # functions for mixture models
library(mixtools)  # poisregmixEM() function
library(readr)
```


# Problem 1: Customer Segmentation with RFM (Recency, Frequency, and Monetary Value)

RFM analysis is an approach that some businesses use to understand their customers' activities. At any point in time, a company can measure how recently a customer purchased a product (Recency), how many times they purchased a product (Frequency), and how much they have spent (Monetary Value). There are many ad-hoc attempts to segment/cluster customers based on the RFM scores (e.g., here is one based on using the customers' rank of each dimension independently: <https://joaocorreia.io/blog/rfm-analysis-increase-sales-by-segmenting-your-customers.html>). In this problem you will use the clustering methods we covered in class to segment the customers. 


The data for this problem can be found here: <`r file.path(data.dir, "RFM.csv")`>. Cluster based on the Recency, Frequency, and Monetary value columns.


## a. Implement hierarchical clustering. 

- Describe any pre-processing steps you took (e.g., scaling, distance metric)
- State the linkage method you used with justification. 
- Show the resulting dendrogram
- State the number of segments/clusters you used with justification. 
- Using your segmentation, are customers 1 and 100 in the same cluster?     
    
### {.solution}

```{r message=FALSE, warning=FALSE}
data.1a.url = "https://mdporter.github.io/SYS6018/data//RFM.csv"
dataReader = readr::read_csv(data.1a.url)

RFMDataset = dataReader %>% select(Recency,Frequency,Monetary) %>% scale()

dX = dist(RFMDataset,method = "euclidean")
hc.average = hclust(dX,method = "average")    # "ward.D", "single", "complete", "average", "mcquitty", "median", "centroid", "ward.D2"
hc.complete = hclust(dX,method = "complete")   
hc.single = hclust(dX,method = "single")    
hc.centroid = hclust(dX,method = "centroid")   
hc.wardD = hclust(dX,method = "ward.D")    

par(mfrow = c(2,3))
plot(hc.average,sub = "average")
plot(hc.complete,sub = "complete")
plot(hc.single,sub = "single")
plot(hc.centroid,sub="centroid")
plot(hc.wardD,sub = "ward.D")
```

- Describe any pre-processing steps you took (e.g., scaling, distance metric)

  In the pre-processing step, since different column data have different range, first rescale the dataset with function **scale()**. And then calculate the **distance metric** used for the dendrogram plot. 


- State the linkage method you used with justification.

  Compare the five different linkage method, **complete** method would be the best one since it it more balanced compare with the other models. 
  
- Show the resulting dendrogram

```{r}
plot(hc.complete,sub = "complete")

tibble(height = hc.complete$height, K = row_number(-height)) %>% ggplot(aes(K,height)) + geom_line() + geom_point(aes(color = ifelse(K==9,"red","black"))) + scale_color_identity() + coord_cartesian(xlim = c(1,50))
```

- State the number of segments/clusters you used with justification.

  From the plot above, we choose $K=9$ since after $K=9$ the height changes becomes flatten. 


- Using your segmentation, are customers 1 and 100 in the same cluster?

```{r}
y = cutree(hc.complete,k = 9)
res.1a = tibble(customer=numeric(), cluster = numeric())
res.1a = add_row(res.1a, tibble(customer = 1, cluster = y[1]))
res.1a = add_row(res.1a, tibble(customer = 100, cluster = y[100]))

res.1a %>% knitr::kable()
```
Customer 1 is in cluster 1, while customer 100 is in cluster 2.


## b. Implement k-means.  

- Describe any pre-processing steps you took (e.g., scaling)
- State the number of segments/clusters you used with justification. 
- Using your segmentation, are customers 1 and 100 in the same cluster?     
    
### {.solution}

```{r warning=FALSE}
KmeanDataset = dataReader %>% select(Recency,Frequency,Monetary) %>% scale()

Kmax = 30
SSE = numeric(Kmax)

for (k in 1:Kmax) {
  km = kmeans(KmeanDataset,centers = k, nstart = 2)
  SSE[k] = km$tot.withinss
}

# tibble(K = 1:Kmax, sse = SSE) %>%
#   ggplot(aes(x = K,y = sse)) + geom_point() + geom_line() + labs(title = "K-means for RFM") + geom_point(aes(color = ifelse(K==6,"red","black"))) + scale_color_identity() + coord_cartesian(xlim = c(1,30))
```

- Describe any pre-processing steps you took (e.g., scaling)

  In the pre-processing step, since different column data have different range, first rescale the dataset with function scale(). 

- State the number of segments/clusters you used with justification. 
  In the above plot, several value of $K$ are used to evaluate the SSE. The K value at **elbow** point is **$6$** 
```{r}
tibble(K = 1:Kmax, sse = SSE) %>%
  ggplot(aes(x = K,y = sse)) + geom_point() + geom_line() + labs(title = "K-means for RFM") + geom_point(aes(color = ifelse(K==6,"red","black"))) + scale_color_identity() + coord_cartesian(xlim = c(1,30))
```
  
- Using your segmentation, are customers 1 and 100 in the same cluster?

  From the table below we can see, customer $1$ is in group 4, while as customer $100$ in group %2%. They are in different cluster. 
```{r}
set.seed(2021)
y = kmeans(KmeanDataset, centers = 6, nstart = 2)

res.1b = tibble(customer=numeric(), cluster = numeric())
res.1b = add_row(res.1b, tibble(customer = 1, cluster = y$cluster[1]))
res.1b = add_row(res.1b, tibble(customer = 100, cluster = y$cluster[100]))

res.1b %>% knitr::kable()
```



## c. Implement model-based clustering

- Describe any pre-processing steps you took (e.g., scaling)
- State the number of segments/clusters you used with justification. 
- Describe the best model. What restrictions are on the shape of the components?
- Using your segmentation, are customers 1 and 100 in the same cluster?     

### {.solution}
```{r}
modelBasedDataSet  =  dataReader %>% select(Recency,Frequency,Monetary) %>% scale()

mix = Mclust(modelBasedDataSet,verbose = FALSE)
summary(mix,parameters = TRUE)

plot(mix,what = "BIC")
plot(mix,what = "classification")
plot(mix,what = "density")
```

- Describe any pre-processing steps you took (e.g., scaling)
  
  In the pre-processing step, since different column data have different range, first rescale the dataset with function scale().

- State the number of segments/clusters you used with justification. 
  
  8 clusters used in the justification. 

- Describe the best model. What restrictions are on the shape of the components?
  
  The Best model is `r mix$modelName`
  

- Using your segmentation, are customers 1 and 100 in the same cluster?
  From the table we can see, customer $1$ is in cluster `r mix$classification[1]`, while as customer $100$ in custer `r  mix$classification[100]`
  
```{r}
res.1c = tibble(customer=numeric(), cluster = numeric())
res.1c = add_row(res.1c, tibble(customer = 1, cluster = mix$classification[1]))
res.1c = add_row(res.1c, tibble(customer = 100, cluster = mix$classification[100]))
res.1c %>% knitr::kable()
```



## d. Discuss how you would cluster the customers if you had to do this for your job. Do you think one model would do better than the others? 

### {.solution}
   For problems likes crab classification, in the dataset there will be an "true" value for the problem. This would make the judgment easier since we could compare the model with the true cluster, the model with higher accuracy would be the model we choose. For customer clustering problem, there is no direct criteria that we can make the judgment. While as, if the dataset is reasonably sampled, the number of the cluster should not too large or too small like only 1 or two cluster or over hundreds of clusters. All the models gives $K$ value around $5-9$ which is reasonable, we can not see a clear sign which model will beat the others for sure, either one could be the best in practice. If we donot have more input to varify the performance of the models, a reasonable way to testify those models would be put the model in real world, probably predict the similarity of the customer in the future shoping to find the best model. 





# Problem 2: Poisson Mixture Model

The pmf of a Poisson random variable is:
\begin{align*}
f_k(x; \lambda_k) = \frac{\lambda_k^x e^{-\lambda_k}}{x!}
\end{align*}

A two-component Poisson mixture model can be written:
\begin{align*}
f(x; \theta) = \pi \frac{\lambda_1^x e^{-\lambda_1}}{x!} + (1-\pi) \frac{\lambda_2^x e^{-\lambda_2}}{x!}
\end{align*}



## a. What are the parameters of the model? 

### {.solution}

The parameters of the model : $\lambda_1$, $\lambda_2$, $\pi$


## b. Write down the log-likelihood for $n$ independent observations ($x_1, x_2, \ldots, x_n$). 

### {.solution}

$$L(x,\theta) = \prod_{i=1}^nf(x_i,\theta)$$
$$log(L(x,\theta)) = \sum_i^nlog(\pi\frac{\lambda_1^{x_i}e^{-\lambda_1}}{{x_i}!} + (1-\pi)\frac{\lambda_2^{x_i}e^{-\lambda_2}}{x_i!})$$
$$log(L(x,\theta)) = \sum_i^nlog(\pi\lambda_1^{x_i}e^{-\lambda_1} + (1-\pi)\lambda_2^{x_i}e^{-\lambda_2}) - \sum_i^nlog({x_i}!)$$

## c. Suppose we have initial values of the parameters. Write down the equation for updating the *responsibilities*. 

### {.solution}

$$r_{i1} = \frac{\pi\frac{\lambda_1^{x_i}e^{-\lambda_1}}{x_i!}}{\pi\frac{\lambda_1^{x_i}e^{-\lambda_1}}{x_i!} + (1-\pi)\frac{\lambda_2^{x_i}e^{-\lambda_2}}{x_i!}} = \frac{\pi\lambda_1^{x_i}e^{-\lambda_1}}{\pi\lambda_1^{x_i}e^{-\lambda_1} + (1-\pi)\lambda_2^{x_i}e^{-\lambda_2}}$$

$$r_{i1} = \frac{(1-\pi)\frac{\lambda_2^{x_i}e^{-\lambda_2}}{x_i!}}{\pi\frac{\lambda_1^{x_i}e^{-\lambda_1}}{x_i!} + (1-\pi)\frac{\lambda_2^{x_i}e^{-\lambda_2}}{x_i!}} = \frac{(1-\pi)\lambda_2^{x_i}e^{-\lambda_2}}{\pi\lambda_1^{x_i}e^{-\lambda_1} + (1-\pi)\lambda_2^{x_i}e^{-\lambda_2}}$$


## d. Suppose we have responsibilities, $r_{ik}$ for all $i=1, 2, \ldots, n$ and $k=1,2$. Write down the equations for updating the parameters. 

### {.solution}

$$\hat{n}_k = \sum_{i=1}^n{r_{ik}}$$
$$\hat{\pi}_k = \frac{\hat{n}_k}{n}$$
$$\hat{\lambda}_k = \frac{\sum^n_{i=1}{(r_{ik}x_i)}}{\hat{n}_k}$$



## e. Fit a two-component Poisson mixture model, report the estimated parameter values, and show a plot of the estimated mixture pmf for the following data:

```{r, echo=TRUE}
#-- Run this code to generate the data
set.seed(123)             # set seed for reproducibility
n = 200                   # sample size
z = sample(1:2, size=n, replace=TRUE, prob=c(.25, .75)) # sample the latent class
theta = c(8, 16)          # true parameters
y = ifelse(z==1, rpois(n, lambda=theta[1]), rpois(n, lambda=theta[2]))
```


<div style="background-color:lightgrey; display: block; border-color: black; padding:1em">

Note: The function `poisregmixEM()` in the R package `mixtools` is designed to estimate a mixture of *Poisson regression* models. We can still use this function for our problem of density estimation if it is recast as an intercept-only regression. To do so, set the $x$ argument (predictors) to `x = rep(1, length(y))` and `addintercept = FALSE`. 

Look carefully at the output from this model. The `beta` values (regression coefficients) are on the log scale.

</div>


### {.solution}

```{r message=FALSE, warning=FALSE}
x = rep(1, length(y))
# tibble(x=y) %>%
# ggplot(aes(x=x)) + geom_density()
pres.mix = poisregmixEM(y,x,addintercept = FALSE, k =2)
plot(pres.mix)
```

```{r}
pres.mix.res.pi.1 = pres.mix$lambda[1]
pres.mix.res.pi.2 = pres.mix$lambda[2]
pres.mix.res.lambda.1 = exp(pres.mix$beta[1])
pres.mix.res.lambda.2 = exp(pres.mix$beta[2])

fitline.dataset = tibble(x.seq = 0:30,
                  fitline.mix =  pres.mix.res.pi.1 * dpois(x.seq,pres.mix.res.lambda.1) + pres.mix.res.pi.2 * dpois(x.seq,pres.mix.res.lambda.2),
                  fitline.1   =  pres.mix.res.pi.1 * dpois(x.seq,pres.mix.res.lambda.1),
                  fitline.2   = pres.mix.res.pi.2 * dpois(x.seq,pres.mix.res.lambda.2))


# tibble(x=y)%>% ggplot(aes(x))+ geom_histogram(binwidth = 0.6) + geom_line(data = fitline.dataset,aes(y=fitline.mix))
ggplot(fitline.dataset, aes(x=x.seq)) +  
  geom_line(aes(y=fitline.mix,color= "mixture model")) +
  geom_line(aes(y=fitline.1,color= "mix_peak1")) +
  geom_line(aes(y=fitline.2,color= "mix_peak2"))
```
- **Fit Parameters** 

  * $\pi =$ `r pres.mix$lambda[1]`
  * $1 - \pi =$ `r pres.mix$lambda[2]`
  * $\lambda_1 =$ `r exp(pres.mix$beta[1])`
  * $\lambda_2 =$ `r exp(pres.mix$beta[2])`
  
  
## f. **2 pts Extra Credit**: Write a function that estimates this two-component Poisson mixture model using the EM approach. Show that it gives the same result as part *e*. 
- Note: you are not permitted to copy code.  Write everything from scratch and use comments to indicate how the code works (e.g., the E-step, M-step, initialization strategy, and convergence should be clear). 
- Cite any resources you consulted to help with the coding. 

### {.solution}

Add Solution Here


