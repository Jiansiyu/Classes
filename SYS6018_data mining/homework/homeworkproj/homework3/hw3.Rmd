---
title: "Homework #3: Penalized Regression" 
author: "**Siyu Jian / sj9va**"
date: "Due: Wed Mar 3 | 10:55am"
output: R6018::homework
---

**SYS 4582/6018 | Spring 2021 | University of Virginia **

*******************************************
```{r config, echo=FALSE}
source(system.file("config/hw_config.R", package="R6018")) # knitr settings
# options(dplyr.summarise.inform = FALSE)  # ignore dplyr message about grouping
```

# Required R packages and Directories

### {.solution}
```{r packages, message=FALSE, warning=FALSE}
data.dir = 'https://mdporter.github.io/SYS6018/data/' # data directory
library(mlbench)
library(glmnet)
library(R6018)     # functions for SYS-6018
library(tidyverse) # functions for data manipulation   
library(knitr)
```


# Problem 1: Optimal Tuning Parameters

In cross-validation, we discussed choosing the tuning parameter values that minimized the cross-validation error. Another approach, called the "one-standard error" rule [ISL pg 214, ESL pg 61], uses the values corresponding to the least complex model whose cv error is within one standard error of the best model. The goal of this assignment is to compare these two rules.

Use simulated data from `mlbench.friedman1(., sd=2)` in the `mlbench` R package to fit *lasso models*. The tuning parameter $\lambda$ (corresponding to the penalty on the coefficient magnitude) is the one we will focus one. Generate training data, use k-fold cross-validation to get $\lambda_{\rm min}$ and $\lambda_{\rm 1SE}$, generate test data, make predictions for the test data, and compare performance of the two rules under a squared error loss using a hypothesis test.  


Choose reasonable values for:

- Number of cv folds ($K$) 
    - Note: you are free to use repeated CV, repeated hold-outs, or bootstrapping instead of plain cross-validation; just be sure to describe what do did so it will be easier to grade.
- Number of training and test observations
- Number of simulations
- If everyone uses different values, we will be able to see how the results change over the different settings.
- Don't forget to make your results reproducible (e.g., set seed)

This pseudo code will get you started:
```yaml
library(mlbench)
library(glmnet)

#-- Settings
n.train =        # number of training obs
n.test =         # number of test obs
K =              # number of CV folds
alpha =          # glmnet tuning alpha (1 = lasso, 0 = ridge)
M =              # number of simulations

#-- Data Generating Function
getData <- function(n) mlbench.friedman1(n, sd=2) # data generating function

#-- Simulations
# Set Seed Here

for(m in 1:M) {

# 1. Generate Training Data
# 2. Build Training Models using cross-validation, e.g., cv.glmnet()
# 3. get lambda that minimizes cv error and 1 SE rule
# 4. Generate Test Data
# 5. Predict y values for test data (for each model: min, 1SE)
# 6. Evaluate predictions

}

#-- Compare
# compare performance of the approaches / Statistical Test
```

## a. Code for the simulation and performance results

### {.solution}

Add solution here
```{r}
# idea: choose k range, do cv or boostrapping scan on the k. 
# create M simulations
# each simulation lasso fit to get the minimum lamda



library(mlbench)
library(glmnet)

#-- Settings
n.train =10000        # number of training obs
n.test =10000         # number of test obs
K = 10             # number of CV folds
alpha = 1         # glmnet tuning alpha (1 = lasso, 0 = ridge)
M = 200             # number of simulations

#-- Data Generating Function
getData <- function(n) mlbench.friedman1(n, sd=2) # data generating function

#-- Simulations
# Set Seed Here
set.seed(200)

cvRes.1a=tibble(model=str_c(), lambda = numeric(),mse= numeric())

for (m in 1:M){
  data_train =  getData(n.train)
  x.train = data_train$x
  y.train = data_train$y
  
  #build the training model with the glmnet 
  cvfit = cv.glmnet(x.train,y.train, nfolds = K, alpha = alpha)
  lam.min= cvfit$lambda.min
  lam.1se = cvfit$lambda.1se
  
  # generate the test dataset and get the mean of min.lambda and 1st.lambda on the test dataset
  data_test = getData(n.test)
  
  x.test = data_test$x
  y.test = data_test$y
  
  yhat_min = predict(cvfit, newx = x.test, s="lambda.min", type = "response")
  mse.min = mean((yhat_min - y.test)^2)
  
  yhat_1se =  predict(cvfit, newx = x.test, s="lambda.1se", type = "response")
  mse.1se = mean((yhat_1se - y.test)^2)
  
  cvRes.1a = add_row(cvRes.1a, model = "min",lambda = lam.min, mse = mse.min)
  cvRes.1a = add_row(cvRes.1a, model = "1se", lambda = lam.1se, mse = mse.1se)
}

ggplot(data = cvRes.1a, aes(x = mse, color = model)) + geom_density()
```


## b. Description and results of a hypothesis test comparing $\lambda_{\rm min}$ and $\lambda_{\rm 1SE}$.

### {.solution}

Add solution here
```{r}
minres <-cvRes.1a %>% filter(model == "min")
se1res <-cvRes.1a %>% filter(model == "1se")
# print(res$mse)
t.test(minres$lambda, se1res$lambda)
```

From the two t test result we can see, the p-value is $2e^{-16}$. The $\lambda_{min}$ and $\lambda_{1se}$ are significantly different from each other. Also we can see from the following  plot. The distribution of $\lambda_{min}$ and $\lambda_{1se}$ are located at different positions with mean of $0.0192$ and $0.1875$ respectively.
```{r echo=FALSE}
ggplot(data= cvRes.1a, aes(x = lambda, color = model)) + geom_density()
```


# Problem 2 Prediction Contest: Real Estate Pricing


This problem uses the [realestate-train](https://mdporter.github.io/SYS6018/data/realestate-train.csv) and [realestate-test](https://mdporter.github.io/SYS6018/data/realestate-test.csv) (click on links for data). 

The goal of this contest is to predict sale price (in thousands) (`price` column) using an *elastic net* model. Evaluation of the test data will be based on the root mean squared error ${\rm RMSE}= \sqrt{\frac{1}{m}\sum_i (y_i - \hat{y}_i)^2}$ for the $m$ test set observations. 


## a. Load the data and create necessary data structures for running *elastic net*.
- You are free to use any data transformation or feature engineering
- Note: there are some categorical predictors so at the least you will have to convert those to something numeric (e.g., one-hot or dummy coding). 


### {.solution}

Add solution here
```{r message = FALSE}
library(readr)

set.seed(200)

realestate_train_url = "https://mdporter.github.io/SYS6018/data/realestate-train.csv"
realestate_test_url  = "https://mdporter.github.io/SYS6018/data/realestate-test.csv"

train_reader = readr::read_csv(realestate_train_url)
test_reader = readr::read_csv(realestate_test_url)

data_train.x = model.matrix(~ 0 + PoolArea + GarageCars + Fireplaces + TotRmsAbvGrd + Baths + SqFeet  + BldgType + HouseStyle + condition, data=train_reader)

data_train.y = model.matrix(~ 0 + price, data=train_reader)

data_test.x = model.matrix(~ 0 + PoolArea + GarageCars + Fireplaces + TotRmsAbvGrd + Baths + SqFeet  + BldgType + HouseStyle + condition, data=test_reader)

```


## b. Use an *elastic net* model to predict the `price` of the test data.  
- You are free to use any data transformation or feature engineering
- You are free to use any tuning parameters
- Report the $\alpha$ and $\lambda$ parameters you used to make your final predictions.
- Describe how you choose those tuning parameters

### {.solution}


```{r}

n.fold = 10
fold = sample(rep(1:n.fold, length=nrow(data_train.x)))

alpha.range = seq(0,1,by=0.01)
cvRes.2b = tibble(model = str_c(),mse=numeric(),alpha = numeric(), lambda = numeric())
for ( alpha in alpha.range){
  fit.enet = cv.glmnet(data_train.x,data_train.y, alpha = alpha, foldid = fold)
  mse.min.enet = fit.enet$cvm[fit.enet$lambda == fit.enet$lambda.min]
  mse.1se.enet = fit.enet$cvm[fit.enet$lambda == fit.enet$lambda.1se]
  lambda.min   = fit.enet$lambda.min
  lambda.1se   = fit.enet$lambda.1se

  cvRes.2b = add_row(cvRes.2b, model = "min",alpha = alpha,mse = mse.min.enet, lambda = lambda.min)
  cvRes.2b = add_row(cvRes.2b, model = "1se",alpha = alpha, mse = mse.1se.enet,lambda = lambda.1se)
}

ggplot(cvRes.2b,aes(x = alpha,y=mse,color = model)) + geom_point() + geom_line() + 
  geom_point(data =. %>% filter(mse == min(mse)),color="red",size=3)

```

```{r}
minRes <- cvRes.2b %>% filter(mse == min(mse))
minRes %>% kable()
```

In elastic net penalty, $\alpha$ control the contribution from the linear and qudratic contributions. The idea of the above approach is scan on the $\alpha$,  and on each $\alpha$ do cross validation to find the lambda that achive the best mse value. In this case, we would be able to find the best $\alpha$ value for our model. 

We also compared the perforance of the "1 standard error" rule. We can see the minimum lambda gives better result. 

In this approach, $\alpha=0.2$ and $\lambda=1.114$ get the minimum $mse=1869$.

## c. Submit a .csv file (ensure comma separated format) named `lastname_firstname.csv` that includes the column named *yhat* that is your estimates. We will use automated evaluation, so the format must be exact.  
- You will receive credit for a proper submission; the top five scores will receive 2 bonus points.     
    
### {.solution}

Add solution here
```{r}
alpha_min = minRes$alpha

fit.enet = cv.glmnet(data_train.x,data_train.y, alpha = alpha_min, foldid = fold)

yhat.enet = predict(fit.enet,newx = data_test.x, s="lambda.min",type="response")

colnames(yhat.enet)[1] <- "yhat"
write.csv(yhat.enet,"jian_siyu.csv")

```


## d. Report the anticipated performance of your method in terms of RMSE. We will see how close your performance assessment matches the actual value. 

### {.solution}
```{r}
rmse <- sqrt(fit.enet$cvm[fit.enet$lambda == fit.enet$lambda.min])

kable(tibble(RMSE= rmse))
```
The Expected RMSE of our prediction is $43.23$.

